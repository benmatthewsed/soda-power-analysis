---
title: "SODA Power Analysis"
author: "Ben Matthews"
date: "2023-06-23"
output: pdf_document
---

## Set up



```{r}
library(tidyverse)
source(here::here("script", "xx-functions.R"))
set.seed(nchar("soda power analysis") ^ 3)

```



```{r}
# define parameters

soda_effect_size <- 0.1
# assumes larger soda at t1 gives larger soda at t2
# although we don't use this in the power calculations
# this is a small effect size - around 0.2-0.6 soda points
# (it depends on the location on the scale exactly how much)


miss_prop <- 0.25
# proportion of data at t2 (unrelated to t1 soda)

study_n <- 1000
# sample size

clusters_n <- 4
# number of recruitment conditions

cluster_effect_size <- 0.1
# this is the variance for the cluster random effect, again
# this isn't large (could be around a whole soda point either way)

id_effect_size <- 1
# this is between person variability and is very large
# so like 5 or 6 soda points


intervention_effect <- -1 
# this is change over time
# this is a very large effect size
```

We can generate data with the following function  (see xx-functions.R)



```{r}
tmp <- 
gen_soda_data(
  soda_effect_size = soda_effect_size,  
  miss_prop = miss_prop,
  study_n = study_n,
  clusters_n = 4,
  cluster_effect_size = 0.1,
  id_effect_size = 1,
  intervention_effect = -0.5
)
```

This simulates a t1 soda score from the binomial distribution based on the cluster and individual effects then simulates a t2 soda score from the same distribution using the same cluster and individual effects, plus a small effect of the t1 cluster score.

We then only keep the proportion of the t2 data specified by miss_prop. this missing probability is uncorrelated with any other factors.

As a result soda at t1 and t2 are correlated:

```{r}
cor(tmp |> filter(time == 0) |> pull(soda),
    tmp |> filter(time == 1) |> pull(soda),
    use = "complete.obs")
```

## P-values

For the analysis model we use:

```{r}

glm_mod <- 
  tmp |> 
  MASS::glm.nb(soda ~ time, data = _)

# and then extract the p-value

broom::tidy(glm_mod)$p.value[[2]]

```

This just models SODA as a count variable. We don't account for clustering within individuals within this model. we could do using:

```{r}

glmer_mod <- 
  tmp |> 
  lme4::glmer.nb(soda ~ time + (1 | id), data = _)

# and then extract the p-value with

broom.mixed::tidy(glmer_mod)$p.value[[2]]

```

This actually gives more precise estimates, possibly because the data were generated with an individual random effect?

As a result I go for the more conservative model.

## Effect sizes

We can calculate some example effect sizes to see what might be plausible results. I try -0.1, -0.25, -0.5 and -1:

### Effect size -0.1

```{r}
sims_0.1 <- 
tibble(
  sim_n = seq(1:1000)
) |> 
  mutate(sim = map(sim_n, ~ gen_soda_data(
    soda_effect_size = soda_effect_size,  
    miss_prop = miss_prop,
    study_n = study_n,
    clusters_n = 4,
    cluster_effect_size = 0.1,
    id_effect_size = 1,
    intervention_effect = -0.1
  )))


plot_effect_size(sims_0.1)

calc_group_change(sims_0.1)
```

Effect size of -0.1 would give:

- ~20% in group 2 down to 1
- around 22% in group 3 down to 2 or 1
- around 50% of people in group 4 dropping down
- and an average difference of around 0.4 points

Note that some will also go up from group 1 to group 2 etc.

### Effect size -0.25

```{r}
sims_0.25 <- 
  tibble(
    sim_n = seq(1:1000)
  ) |> 
  mutate(sim = map(sim_n, ~ gen_soda_data(
    soda_effect_size = soda_effect_size,  
    miss_prop = miss_prop,
    study_n = study_n,
    clusters_n = 4,
    cluster_effect_size = 0.1,
    id_effect_size = 1,
    intervention_effect = -0.25
  )))

plot_effect_size(sims_0.25)

calc_group_change(sims_0.25)
```


Effect size of -0.25 would give:

- ~27% in group 2 down to 1
- around 30% in group 3 down to 2 or 1
- and around 60% of people in group 4 dropping down
- and an average difference of around 0.8 points


### Effect size -0.5

```{r}
sims_0.5 <- 
  tibble(
    sim_n = seq(1:1000)
  ) |> 
  mutate(sim = map(sim_n, ~ gen_soda_data(
    soda_effect_size = soda_effect_size,  
    miss_prop = miss_prop,
    study_n = study_n,
    clusters_n = 4,
    cluster_effect_size = 0.1,
    id_effect_size = 1,
    intervention_effect = -0.5
  )))

plot_effect_size(sims_0.5)

calc_group_change(sims_0.5)
```


Effect size of -0.5 would give:

- ~39% in group 2 down to 1
- around 42% in group 3 down to 2 or 1
- and around 70% of people in group 4 dropping down
- and an average difference of around 1.7 points


### Effect size -1

```{r}
sims_1 <- 
  tibble(
    sim_n = seq(1:1000)
  ) |> 
  mutate(sim = map(sim_n, ~ gen_soda_data(
    soda_effect_size = soda_effect_size,  
    miss_prop = miss_prop,
    study_n = study_n,
    clusters_n = 4,
    cluster_effect_size = 0.1,
    id_effect_size = 1,
    intervention_effect = -1
  )))


plot_effect_size(sims_1)

calc_group_change(sims_1)
```


Effect size of -1 would give:

- ~60% in group 2 down to 1
- around 70% in group 3 down to 2 or 1
- and around 87% of people in group 4 dropping down
- and around 3.3 points difference on average


## Power analysis

Based on the above I run a power analysis with effect sizes of 0.1, 0.25 and 0.5, assuming that an effect size of 1 is unlikely.

The code below runs the analysis and saves the result to disk. 1000 simulations took around 22 minutes.

```{r, eval = FALSE}

res <- 
tidyr::crossing(
  effect_size = c(-0.1, -0.25, -0.5),
  miss_prop = c(0.1, 0.25, 0.5),
  study_n = c(500, 1000, 1500)
) |> 
  mutate(res = pmap(list(
    a = effect_size,
    b = miss_prop,
    c = study_n),
    \(a, b, c,  ...) 
    run_power_soda(
               sim_n = 1000,
               soda_effect_size = soda_effect_size,  
               miss_prop = b,
               study_n = c,
               clusters_n = 4,
               cluster_effect_size = 0.1,
               id_effect_size = 1,
             intervention_effect = a)))

saveRDS(res,
        here::here("results",
                   "soda-power-analysis_1000.rds"))

```

The results of the power analysis are presented below. A horizontal line is placed on the graph at power of 0.8. It calculates achieved N as a combination of the sample size and the proportion of missing cases.

An effect size of 0.5 could be detected with power larger than 0.8 in the smallest combination of N and largest proportion missing (500 and 11% - 55 cases.)

For an effect size of -0.25 it took until an achieved N of 200 to reach a power of 0.8.

Effect size of -0.1 never achieved a power of 0.8 in these simulations.

```{r}

res <- readRDS(here::here("results",
                          "soda-power-analysis_1000.rds"))

res |> 
  unnest(res) |> 
  group_by(effect_size, miss_prop, study_n) |> 
  summarise(power = weighted.mean(signif, n)) |> 
  ungroup() |> 
  mutate(achieved_n = study_n * miss_prop) |> 
  ggplot(aes(x = achieved_n, 
             y = power, 
             colour = factor(effect_size))) +
  geom_hline(yintercept = 0.8) +
  geom_point(aes(size = factor(study_n))) +
  geom_line(aes(group = effect_size)) +
  labs(x = "Achieved N",
       y = "Power",
       colour = "Effect size",
       size = "Study N")

```